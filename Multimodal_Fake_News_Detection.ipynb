{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tasnimislamraisa/Python_Learning/blob/deep-Learning/Multimodal_Fake_News_Detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BvLE7qZJ_ZYm",
        "outputId": "7e72c0fd-fbba-4043-de3e-a9750e05ebfc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {
        "id": "Krr3mrPVLYsr"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers torchvision scikit-learn\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {
        "id": "jwqqQ8A6LdYT"
      },
      "outputs": [],
      "source": [
        "import os, json, random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from torchvision import transforms, models\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from tqdm import tqdm\n",
        "\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OdoQ3RdAc4JT",
        "outputId": "e39fc922-4a67-4def-9e03-5c3fc3cef337"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Reproducibility\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(42)"
      ],
      "metadata": {
        "id": "mIsM8vPeoYg3"
      },
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xs5N4ef8_95O"
      },
      "source": [
        "# **Set dataset paths in Colab**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {
        "id": "9WDDzAh3IbDu"
      },
      "outputs": [],
      "source": [
        "BASE_DIR = \"/content/drive/MyDrive/FakeNews_Multimodal_Dataset/fakeddit_subset\"\n",
        "\n",
        "TRAIN_JSON = os.path.join(BASE_DIR, \"training_data_fakeddit.jsonl\")\n",
        "VAL_JSON   = os.path.join(BASE_DIR, \"validation_data_fakeddit.jsonl\")\n",
        "\n",
        "TRAIN_IMG_DIR = os.path.join(BASE_DIR, \"image_folder\")\n",
        "VAL_IMG_DIR   = os.path.join(BASE_DIR, \"validation_image\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uo1Gtr1OIkfL",
        "outputId": "7f6b6cef-2e21-4473-b4a2-2d5b3817c41f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAIN_JSON exists: True\n",
            "VAL_JSON exists: True\n",
            "Train images: 4000\n",
            "Val images: 4210\n"
          ]
        }
      ],
      "source": [
        "print(\"TRAIN_JSON exists:\", os.path.exists(TRAIN_JSON))\n",
        "print(\"VAL_JSON exists:\", os.path.exists(VAL_JSON))\n",
        "print(\"Train images:\", len(os.listdir(TRAIN_IMG_DIR)))\n",
        "print(\"Val images:\", len(os.listdir(VAL_IMG_DIR)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LO2mRN_SI-EV"
      },
      "source": [
        "# **Load JSON files**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t4PH8kQiJAbB",
        "outputId": "f8b3dedd-a499-455c-ab25-384deba3733a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Raw train columns: Index(['contents'], dtype='object')\n",
            "Raw val columns: Index(['contents'], dtype='object')\n"
          ]
        }
      ],
      "source": [
        "def load_jsonl(path):\n",
        "    rows = []\n",
        "    with open(path, \"r\") as f:\n",
        "        for line in f:\n",
        "            rows.append(json.loads(line))\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "train_df_raw = load_jsonl(TRAIN_JSON)\n",
        "val_df_raw   = load_jsonl(VAL_JSON)\n",
        "\n",
        "print(\"Raw train columns:\", train_df_raw.columns)\n",
        "print(\"Raw val columns:\", val_df_raw.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {
        "id": "6odxXvh-avJD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be76f8ad-a3ae-4fee-864b-23e7dcc26b73"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parsed train: (4000, 3) Parsed val: (4210, 3)\n",
            "label\n",
            "0    2291\n",
            "1    1709\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "def parse_contents(df):\n",
        "    records = []\n",
        "    for row in df[\"contents\"]:\n",
        "        # each row is like: [ {role:user, parts:[fileData,text]}, {role:model, parts:[text]} ]\n",
        "        user_part = row[0]\n",
        "        model_part = row[1]\n",
        "\n",
        "        image = None\n",
        "        text = None\n",
        "\n",
        "        for part in user_part.get(\"parts\", []):\n",
        "            if \"fileData\" in part:\n",
        "                image = part[\"fileData\"][\"fileUri\"].split(\"/\")[-1]\n",
        "            if \"text\" in part:\n",
        "                text = part[\"text\"]\n",
        "\n",
        "        label_text = model_part[\"parts\"][0][\"text\"].strip().lower()\n",
        "        label = 1 if label_text == \"yes\" else 0\n",
        "\n",
        "        if image and text:\n",
        "            records.append({\"image\": image, \"text\": text, \"label\": label})\n",
        "    return pd.DataFrame(records)\n",
        "\n",
        "train_df = parse_contents(train_df_raw)\n",
        "val_df   = parse_contents(val_df_raw)\n",
        "\n",
        "print(\"Parsed train:\", train_df.shape, \"Parsed val:\", val_df.shape)\n",
        "print(train_df[\"label\"].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zdvRC2XbdH8d",
        "outputId": "c742db5c-c07f-4e87-d324-bc1ec6ec14b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class weights [Real, Fake]: [0.8729812502861023, 1.1702749729156494]\n"
          ]
        }
      ],
      "source": [
        "labels_np = train_df[\"label\"].values\n",
        "class_weights = compute_class_weight(class_weight=\"balanced\", classes=np.array([0,1]), y=labels_np)\n",
        "class_weights = torch.tensor(class_weights, dtype=torch.float32).to(DEVICE)\n",
        "print(\"Class weights [Real, Fake]:\", class_weights.tolist())\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.ColorJitter(0.2, 0.2, 0.2, 0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                         [0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                         [0.229, 0.224, 0.225]),\n",
        "])"
      ],
      "metadata": {
        "id": "cNNJjXZ2pWN9"
      },
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def clean_text(t):\n",
        "    m = re.search(r'Title:\"(.*?)\"', t, flags=re.IGNORECASE|re.DOTALL)\n",
        "    if m:\n",
        "        return m.group(1).strip()\n",
        "    return t.strip()\n",
        "\n",
        "train_df[\"text\"] = train_df[\"text\"].apply(clean_text)\n",
        "val_df[\"text\"]   = val_df[\"text\"].apply(clean_text)\n"
      ],
      "metadata": {
        "id": "IS4r1aY1bIdx"
      },
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
      ],
      "metadata": {
        "id": "3_G4-NvPpbPE"
      },
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "metadata": {
        "id": "_rbm9x3GOX0K"
      },
      "outputs": [],
      "source": [
        "class MultimodalDataset(Dataset):\n",
        "    def __init__(self, df, image_dir, tokenizer, transform, max_len=128):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.image_dir = image_dir\n",
        "        self.tokenizer = tokenizer\n",
        "        self.transform = transform\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "\n",
        "        # Text\n",
        "        enc = self.tokenizer(\n",
        "            row[\"text\"],\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            max_length=self.max_len,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        # Image (safe)\n",
        "        img_path = os.path.join(self.image_dir, row[\"image\"])\n",
        "        try:\n",
        "            img = Image.open(img_path).convert(\"RGB\")\n",
        "        except Exception:\n",
        "            img = Image.new(\"RGB\", (224, 224), (0, 0, 0))\n",
        "\n",
        "        img = self.transform(img)\n",
        "\n",
        "        return {\n",
        "            \"image\": img,\n",
        "            \"input_ids\": enc[\"input_ids\"].squeeze(0),\n",
        "            \"attention_mask\": enc[\"attention_mask\"].squeeze(0),\n",
        "            \"label\": torch.tensor(row[\"label\"], dtype=torch.long)\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQGLqGP5ItcR"
      },
      "source": [
        "**snopes_medical data**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1bW0z7JuANQx"
      },
      "source": [
        "# **Load and inspect the dataset**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 8\n",
        "NUM_WORKERS = 2\n",
        "\n",
        "train_dataset = MultimodalDataset(train_df, TRAIN_IMG_DIR, tokenizer, train_transform)\n",
        "val_dataset   = MultimodalDataset(val_df,   VAL_IMG_DIR,   tokenizer, val_transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
        "val_loader   = DataLoader(val_dataset,   batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)"
      ],
      "metadata": {
        "id": "ze2nsL5XpmDs"
      },
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IpTLiLm8AXPt"
      },
      "source": [
        "***Clean & preprocess***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VpUZjg_FAhEx"
      },
      "source": [
        "# **Trainâ€“Validation Split**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "metadata": {
        "id": "9Uc58Pz5ArEW"
      },
      "outputs": [],
      "source": [
        "class MultimodalFakeNewsModel(nn.Module):\n",
        "    def __init__(self, dropout=0.4):\n",
        "        super().__init__()\n",
        "\n",
        "        # Text encoder\n",
        "        self.bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "        # Image encoder\n",
        "       # self.cnn = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
        "        # Replace classifier head to get 256-dim image features\n",
        "        #self.cnn.fc = nn.Linear(512, 256)\n",
        "\n",
        "        from torchvision.models import efficientnet_b0, EfficientNet_B0_Weights\n",
        "\n",
        "        self.cnn = efficientnet_b0(weights=EfficientNet_B0_Weights.IMAGENET1K_V1)\n",
        "        in_f = self.cnn.classifier[1].in_features\n",
        "        self.cnn.classifier[1] = nn.Linear(in_f, 256)\n",
        "\n",
        "\n",
        "        # Project text CLS -> 256\n",
        "        self.text_fc = nn.Linear(768, 256)\n",
        "\n",
        "        # Fusion: concat -> 512, then gate\n",
        "        self.gate = nn.Sequential(\n",
        "            nn.Linear(512, 256),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(512, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(256, 2)\n",
        "        )\n",
        "\n",
        "    def forward(self, image, input_ids, attention_mask):\n",
        "        bert_out = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        text_cls = bert_out.last_hidden_state[:, 0, :]  # CLS embedding\n",
        "        text_feat = self.text_fc(text_cls)              # (B,256)\n",
        "\n",
        "        img_feat = self.cnn(image)                      # (B,256)\n",
        "\n",
        "        fused = torch.cat([text_feat, img_feat], dim=1) # (B,512)\n",
        "        g = self.gate(fused)                            # (B,256)\n",
        "        fused = torch.cat([text_feat * g, img_feat * (1 - g)], dim=1)  # gated mixing (B,512)\n",
        "\n",
        "        return self.classifier(fused)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = MultimodalFakeNewsModel().to(DEVICE)"
      ],
      "metadata": {
        "id": "ulWEsZczpuf5"
      },
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "emnvWjQuJVKB",
        "outputId": "ee312952-a4f8-4736-edbc-7a92481b281d"
      },
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MultimodalFakeNewsModel(\n",
            "  (bert): BertModel(\n",
            "    (embeddings): BertEmbeddings(\n",
            "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
            "      (position_embeddings): Embedding(512, 768)\n",
            "      (token_type_embeddings): Embedding(2, 768)\n",
            "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (encoder): BertEncoder(\n",
            "      (layer): ModuleList(\n",
            "        (0-11): 12 x BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSdpaSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (pooler): BertPooler(\n",
            "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (activation): Tanh()\n",
            "    )\n",
            "  )\n",
            "  (cnn): EfficientNet(\n",
            "    (features): Sequential(\n",
            "      (0): Conv2dNormActivation(\n",
            "        (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): SiLU(inplace=True)\n",
            "      )\n",
            "      (1): Sequential(\n",
            "        (0): MBConv(\n",
            "          (block): Sequential(\n",
            "            (0): Conv2dNormActivation(\n",
            "              (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
            "              (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (2): SiLU(inplace=True)\n",
            "            )\n",
            "            (1): SqueezeExcitation(\n",
            "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
            "              (fc1): Conv2d(32, 8, kernel_size=(1, 1), stride=(1, 1))\n",
            "              (fc2): Conv2d(8, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "              (activation): SiLU(inplace=True)\n",
            "              (scale_activation): Sigmoid()\n",
            "            )\n",
            "            (2): Conv2dNormActivation(\n",
            "              (0): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            )\n",
            "          )\n",
            "          (stochastic_depth): StochasticDepth(p=0.0, mode=row)\n",
            "        )\n",
            "      )\n",
            "      (2): Sequential(\n",
            "        (0): MBConv(\n",
            "          (block): Sequential(\n",
            "            (0): Conv2dNormActivation(\n",
            "              (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (2): SiLU(inplace=True)\n",
            "            )\n",
            "            (1): Conv2dNormActivation(\n",
            "              (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
            "              (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (2): SiLU(inplace=True)\n",
            "            )\n",
            "            (2): SqueezeExcitation(\n",
            "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
            "              (fc1): Conv2d(96, 4, kernel_size=(1, 1), stride=(1, 1))\n",
            "              (fc2): Conv2d(4, 96, kernel_size=(1, 1), stride=(1, 1))\n",
            "              (activation): SiLU(inplace=True)\n",
            "              (scale_activation): Sigmoid()\n",
            "            )\n",
            "            (3): Conv2dNormActivation(\n",
            "              (0): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            )\n",
            "          )\n",
            "          (stochastic_depth): StochasticDepth(p=0.0125, mode=row)\n",
            "        )\n",
            "        (1): MBConv(\n",
            "          (block): Sequential(\n",
            "            (0): Conv2dNormActivation(\n",
            "              (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (2): SiLU(inplace=True)\n",
            "            )\n",
            "            (1): Conv2dNormActivation(\n",
            "              (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
            "              (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (2): SiLU(inplace=True)\n",
            "            )\n",
            "            (2): SqueezeExcitation(\n",
            "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
            "              (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
            "              (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
            "              (activation): SiLU(inplace=True)\n",
            "              (scale_activation): Sigmoid()\n",
            "            )\n",
            "            (3): Conv2dNormActivation(\n",
            "              (0): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            )\n",
            "          )\n",
            "          (stochastic_depth): StochasticDepth(p=0.025, mode=row)\n",
            "        )\n",
            "      )\n",
            "      (3): Sequential(\n",
            "        (0): MBConv(\n",
            "          (block): Sequential(\n",
            "            (0): Conv2dNormActivation(\n",
            "              (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (2): SiLU(inplace=True)\n",
            "            )\n",
            "            (1): Conv2dNormActivation(\n",
            "              (0): Conv2d(144, 144, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=144, bias=False)\n",
            "              (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (2): SiLU(inplace=True)\n",
            "            )\n",
            "            (2): SqueezeExcitation(\n",
            "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
            "              (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
            "              (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
            "              (activation): SiLU(inplace=True)\n",
            "              (scale_activation): Sigmoid()\n",
            "            )\n",
            "            (3): Conv2dNormActivation(\n",
            "              (0): Conv2d(144, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            )\n",
            "          )\n",
            "          (stochastic_depth): StochasticDepth(p=0.037500000000000006, mode=row)\n",
            "        )\n",
            "        (1): MBConv(\n",
            "          (block): Sequential(\n",
            "            (0): Conv2dNormActivation(\n",
            "              (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (2): SiLU(inplace=True)\n",
            "            )\n",
            "            (1): Conv2dNormActivation(\n",
            "              (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n",
            "              (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (2): SiLU(inplace=True)\n",
            "            )\n",
            "            (2): SqueezeExcitation(\n",
            "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
            "              (fc1): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
            "              (fc2): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
            "              (activation): SiLU(inplace=True)\n",
            "              (scale_activation): Sigmoid()\n",
            "            )\n",
            "            (3): Conv2dNormActivation(\n",
            "              (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            )\n",
            "          )\n",
            "          (stochastic_depth): StochasticDepth(p=0.05, mode=row)\n",
            "        )\n",
            "      )\n",
            "      (4): Sequential(\n",
            "        (0): MBConv(\n",
            "          (block): Sequential(\n",
            "            (0): Conv2dNormActivation(\n",
            "              (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (2): SiLU(inplace=True)\n",
            "            )\n",
            "            (1): Conv2dNormActivation(\n",
            "              (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=240, bias=False)\n",
            "              (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (2): SiLU(inplace=True)\n",
            "            )\n",
            "            (2): SqueezeExcitation(\n",
            "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
            "              (fc1): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
            "              (fc2): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
            "              (activation): SiLU(inplace=True)\n",
            "              (scale_activation): Sigmoid()\n",
            "            )\n",
            "            (3): Conv2dNormActivation(\n",
            "              (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            )\n",
            "          )\n",
            "          (stochastic_depth): StochasticDepth(p=0.0625, mode=row)\n",
            "        )\n",
            "        (1): MBConv(\n",
            "          (block): Sequential(\n",
            "            (0): Conv2dNormActivation(\n",
            "              (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (2): SiLU(inplace=True)\n",
            "            )\n",
            "            (1): Conv2dNormActivation(\n",
            "              (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
            "              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (2): SiLU(inplace=True)\n",
            "            )\n",
            "            (2): SqueezeExcitation(\n",
            "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
            "              (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
            "              (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
            "              (activation): SiLU(inplace=True)\n",
            "              (scale_activation): Sigmoid()\n",
            "            )\n",
            "            (3): Conv2dNormActivation(\n",
            "              (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            )\n",
            "          )\n",
            "          (stochastic_depth): StochasticDepth(p=0.07500000000000001, mode=row)\n",
            "        )\n",
            "        (2): MBConv(\n",
            "          (block): Sequential(\n",
            "            (0): Conv2dNormActivation(\n",
            "              (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (2): SiLU(inplace=True)\n",
            "            )\n",
            "            (1): Conv2dNormActivation(\n",
            "              (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
            "              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (2): SiLU(inplace=True)\n",
            "            )\n",
            "            (2): SqueezeExcitation(\n",
            "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
            "              (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
            "              (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
            "              (activation): SiLU(inplace=True)\n",
            "              (scale_activation): Sigmoid()\n",
            "            )\n",
            "            (3): Conv2dNormActivation(\n",
            "              (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            )\n",
            "          )\n",
            "          (stochastic_depth): StochasticDepth(p=0.08750000000000001, mode=row)\n",
            "        )\n",
            "      )\n",
            "      (5): Sequential(\n",
            "        (0): MBConv(\n",
            "          (block): Sequential(\n",
            "            (0): Conv2dNormActivation(\n",
            "              (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (2): SiLU(inplace=True)\n",
            "            )\n",
            "            (1): Conv2dNormActivation(\n",
            "              (0): Conv2d(480, 480, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=480, bias=False)\n",
            "              (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (2): SiLU(inplace=True)\n",
            "            )\n",
            "            (2): SqueezeExcitation(\n",
            "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
            "              (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
            "              (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
            "              (activation): SiLU(inplace=True)\n",
            "              (scale_activation): Sigmoid()\n",
            "            )\n",
            "            (3): Conv2dNormActivation(\n",
            "              (0): Conv2d(480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            )\n",
            "          )\n",
            "          (stochastic_depth): StochasticDepth(p=0.1, mode=row)\n",
            "        )\n",
            "        (1): MBConv(\n",
            "          (block): Sequential(\n",
            "            (0): Conv2dNormActivation(\n",
            "              (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (2): SiLU(inplace=True)\n",
            "            )\n",
            "            (1): Conv2dNormActivation(\n",
            "              (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
            "              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (2): SiLU(inplace=True)\n",
            "            )\n",
            "            (2): SqueezeExcitation(\n",
            "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
            "              (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
            "              (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
            "              (activation): SiLU(inplace=True)\n",
            "              (scale_activation): Sigmoid()\n",
            "            )\n",
            "            (3): Conv2dNormActivation(\n",
            "              (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            )\n",
            "          )\n",
            "          (stochastic_depth): StochasticDepth(p=0.1125, mode=row)\n",
            "        )\n",
            "        (2): MBConv(\n",
            "          (block): Sequential(\n",
            "            (0): Conv2dNormActivation(\n",
            "              (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (2): SiLU(inplace=True)\n",
            "            )\n",
            "            (1): Conv2dNormActivation(\n",
            "              (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
            "              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (2): SiLU(inplace=True)\n",
            "            )\n",
            "            (2): SqueezeExcitation(\n",
            "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
            "              (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
            "              (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
            "              (activation): SiLU(inplace=True)\n",
            "              (scale_activation): Sigmoid()\n",
            "            )\n",
            "            (3): Conv2dNormActivation(\n",
            "              (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            )\n",
            "          )\n",
            "          (stochastic_depth): StochasticDepth(p=0.125, mode=row)\n",
            "        )\n",
            "      )\n",
            "      (6): Sequential(\n",
            "        (0): MBConv(\n",
            "          (block): Sequential(\n",
            "            (0): Conv2dNormActivation(\n",
            "              (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (2): SiLU(inplace=True)\n",
            "            )\n",
            "            (1): Conv2dNormActivation(\n",
            "              (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=672, bias=False)\n",
            "              (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (2): SiLU(inplace=True)\n",
            "            )\n",
            "            (2): SqueezeExcitation(\n",
            "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
            "              (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
            "              (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
            "              (activation): SiLU(inplace=True)\n",
            "              (scale_activation): Sigmoid()\n",
            "            )\n",
            "            (3): Conv2dNormActivation(\n",
            "              (0): Conv2d(672, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            )\n",
            "          )\n",
            "          (stochastic_depth): StochasticDepth(p=0.1375, mode=row)\n",
            "        )\n",
            "        (1): MBConv(\n",
            "          (block): Sequential(\n",
            "            (0): Conv2dNormActivation(\n",
            "              (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (2): SiLU(inplace=True)\n",
            "            )\n",
            "            (1): Conv2dNormActivation(\n",
            "              (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
            "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (2): SiLU(inplace=True)\n",
            "            )\n",
            "            (2): SqueezeExcitation(\n",
            "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
            "              (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
            "              (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
            "              (activation): SiLU(inplace=True)\n",
            "              (scale_activation): Sigmoid()\n",
            "            )\n",
            "            (3): Conv2dNormActivation(\n",
            "              (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            )\n",
            "          )\n",
            "          (stochastic_depth): StochasticDepth(p=0.15000000000000002, mode=row)\n",
            "        )\n",
            "        (2): MBConv(\n",
            "          (block): Sequential(\n",
            "            (0): Conv2dNormActivation(\n",
            "              (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (2): SiLU(inplace=True)\n",
            "            )\n",
            "            (1): Conv2dNormActivation(\n",
            "              (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
            "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (2): SiLU(inplace=True)\n",
            "            )\n",
            "            (2): SqueezeExcitation(\n",
            "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
            "              (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
            "              (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
            "              (activation): SiLU(inplace=True)\n",
            "              (scale_activation): Sigmoid()\n",
            "            )\n",
            "            (3): Conv2dNormActivation(\n",
            "              (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            )\n",
            "          )\n",
            "          (stochastic_depth): StochasticDepth(p=0.1625, mode=row)\n",
            "        )\n",
            "        (3): MBConv(\n",
            "          (block): Sequential(\n",
            "            (0): Conv2dNormActivation(\n",
            "              (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (2): SiLU(inplace=True)\n",
            "            )\n",
            "            (1): Conv2dNormActivation(\n",
            "              (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
            "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (2): SiLU(inplace=True)\n",
            "            )\n",
            "            (2): SqueezeExcitation(\n",
            "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
            "              (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
            "              (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
            "              (activation): SiLU(inplace=True)\n",
            "              (scale_activation): Sigmoid()\n",
            "            )\n",
            "            (3): Conv2dNormActivation(\n",
            "              (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            )\n",
            "          )\n",
            "          (stochastic_depth): StochasticDepth(p=0.17500000000000002, mode=row)\n",
            "        )\n",
            "      )\n",
            "      (7): Sequential(\n",
            "        (0): MBConv(\n",
            "          (block): Sequential(\n",
            "            (0): Conv2dNormActivation(\n",
            "              (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (2): SiLU(inplace=True)\n",
            "            )\n",
            "            (1): Conv2dNormActivation(\n",
            "              (0): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)\n",
            "              (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "              (2): SiLU(inplace=True)\n",
            "            )\n",
            "            (2): SqueezeExcitation(\n",
            "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
            "              (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
            "              (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
            "              (activation): SiLU(inplace=True)\n",
            "              (scale_activation): Sigmoid()\n",
            "            )\n",
            "            (3): Conv2dNormActivation(\n",
            "              (0): Conv2d(1152, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "              (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "            )\n",
            "          )\n",
            "          (stochastic_depth): StochasticDepth(p=0.1875, mode=row)\n",
            "        )\n",
            "      )\n",
            "      (8): Conv2dNormActivation(\n",
            "        (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (2): SiLU(inplace=True)\n",
            "      )\n",
            "    )\n",
            "    (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
            "    (classifier): Sequential(\n",
            "      (0): Dropout(p=0.2, inplace=True)\n",
            "      (1): Linear(in_features=1280, out_features=256, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (text_fc): Linear(in_features=768, out_features=256, bias=True)\n",
            "  (gate): Sequential(\n",
            "    (0): Linear(in_features=512, out_features=256, bias=True)\n",
            "    (1): Sigmoid()\n",
            "  )\n",
            "  (classifier): Sequential(\n",
            "    (0): Linear(in_features=512, out_features=256, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Dropout(p=0.4, inplace=False)\n",
            "    (3): Linear(in_features=256, out_features=2, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "metadata": {
        "id": "A-tI4ExPofAz"
      },
      "outputs": [],
      "source": [
        "# -------------------------\n",
        "# BERT: freeze all, unfreeze last 4\n",
        "# -------------------------\n",
        "for p in model.bert.parameters():\n",
        "    p.requires_grad = False\n",
        "\n",
        "for layer in model.bert.encoder.layer[-4:]:\n",
        "    for p in layer.parameters():\n",
        "        p.requires_grad = True\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# EfficientNet: freeze all, unfreeze last blocks + classifier\n",
        "# -------------------------\n",
        "for p in model.cnn.parameters():\n",
        "    p.requires_grad = False\n",
        "\n",
        "# Unfreeze last 2 feature blocks (strong but still stable)\n",
        "for p in model.cnn.features[-2:].parameters():\n",
        "    p.requires_grad = True\n",
        "\n",
        "# Unfreeze classifier head\n",
        "for p in model.cnn.classifier.parameters():\n",
        "    p.requires_grad = True\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# Always train fusion/classifier layers\n",
        "# -------------------------\n",
        "for p in model.text_fc.parameters():\n",
        "    p.requires_grad = True\n",
        "for p in model.gate.parameters():\n",
        "    p.requires_grad = True\n",
        "for p in model.classifier.parameters():\n",
        "    p.requires_grad = True\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, alpha=None, gamma=2.0):\n",
        "        super().__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "\n",
        "    def forward(self, logits, targets):\n",
        "        ce = nn.functional.cross_entropy(logits, targets, reduction='none', weight=self.alpha)\n",
        "        pt = torch.exp(-ce)\n",
        "        loss = ((1 - pt) ** self.gamma) * ce\n",
        "        return loss.mean()\n"
      ],
      "metadata": {
        "id": "auhTo_5kWGXM"
      },
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = FocalLoss(alpha=class_weights, gamma=2.0)\n"
      ],
      "metadata": {
        "id": "fxMnITVyp3fS"
      },
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {
        "id": "XUdCWrPFaVxY"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.AdamW([\n",
        "    {\"params\": model.bert.encoder.layer[-4:].parameters(), \"lr\": 2e-5},\n",
        "\n",
        "    # EfficientNet last blocks + classifier\n",
        "    {\"params\": model.cnn.features[-2:].parameters(),       \"lr\": 1e-4},\n",
        "    {\"params\": model.cnn.classifier.parameters(),          \"lr\": 1e-4},\n",
        "\n",
        "    # Fusion head\n",
        "    {\"params\": model.text_fc.parameters(),                 \"lr\": 1e-4},\n",
        "    {\"params\": model.gate.parameters(),                    \"lr\": 1e-4},\n",
        "    {\"params\": model.classifier.parameters(),              \"lr\": 1e-4},\n",
        "], weight_decay=0.01)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "total = sum(p.numel() for p in model.parameters())\n",
        "print(\"Trainable:\", trainable, \"Total:\", total)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "na7ikN_UW-Uv",
        "outputId": "7f771e7a-cec5-4623-ae79-23b638d892b6"
      },
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trainable: 30268850 Total: 114277758\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer, mode=\"max\", factor=0.5, patience=1\n",
        ")\n"
      ],
      "metadata": {
        "id": "JhbquRo5rIZE"
      },
      "execution_count": 147,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pAAXD34sc-0U"
      },
      "source": [
        "# **Training Loop**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 148,
      "metadata": {
        "id": "XhkYB2BpdJfz"
      },
      "outputs": [],
      "source": [
        "def train_one_epoch(model, loader):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    correct = 0\n",
        "\n",
        "    for batch in tqdm(loader, desc=\"Train\", leave=False):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        image = batch[\"image\"].to(DEVICE)\n",
        "        input_ids = batch[\"input_ids\"].to(DEVICE)\n",
        "        attn = batch[\"attention_mask\"].to(DEVICE)\n",
        "        labels = batch[\"label\"].to(DEVICE)\n",
        "\n",
        "        logits = model(image, input_ids, attn)\n",
        "        loss = criterion(logits, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        correct += (logits.argmax(1) == labels).sum().item()\n",
        "\n",
        "    return total_loss / len(loader), correct / len(loader.dataset)\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_one_epoch(model, loader):\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    correct = 0\n",
        "    all_preds, all_labels = [], []\n",
        "\n",
        "    for batch in tqdm(loader, desc=\"Val\", leave=False):\n",
        "        image = batch[\"image\"].to(DEVICE)\n",
        "        input_ids = batch[\"input_ids\"].to(DEVICE)\n",
        "        attn = batch[\"attention_mask\"].to(DEVICE)\n",
        "        labels = batch[\"label\"].to(DEVICE)\n",
        "\n",
        "        logits = model(image, input_ids, attn)\n",
        "        loss = criterion(logits, labels)\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        preds = logits.argmax(1)\n",
        "\n",
        "        correct += (preds == labels).sum().item()\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    acc = correct / len(loader.dataset)\n",
        "    return total_loss / len(loader), acc, np.array(all_labels), np.array(all_preds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wz5ayJfedQK5"
      },
      "source": [
        "# **Train Model**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 13) TRAIN WITH EARLY STOPPING (ON VAL ACC)\n",
        "# =========================\n",
        "EPOCHS = 10\n",
        "best_val_acc = 0.0\n",
        "patience = 2\n",
        "bad_epochs = 0\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    tr_loss, tr_acc = train_one_epoch(model, train_loader)\n",
        "    va_loss, va_acc, y_true, y_pred = eval_one_epoch(model, val_loader)\n",
        "\n",
        "    print(f\"\\nEpoch {epoch}/{EPOCHS}\")\n",
        "    print(f\"Train Loss: {tr_loss:.4f} | Train Acc: {tr_acc:.4f}\")\n",
        "    print(f\"Val   Loss: {va_loss:.4f} | Val   Acc: {va_acc:.4f}\")\n",
        "\n",
        "    # Step scheduler on val acc\n",
        "    scheduler.step(va_acc)\n",
        "\n",
        "    # Early stopping + save best\n",
        "    if va_acc > best_val_acc:\n",
        "        best_val_acc = va_acc\n",
        "        bad_epochs = 0\n",
        "        torch.save(model.state_dict(), \"best_multimodal_model.pt\")\n",
        "        print(\"âœ… Saved best model.\")\n",
        "    else:\n",
        "        bad_epochs += 1\n",
        "        if bad_epochs >= patience:\n",
        "            print(\"ðŸ›‘ Early stopping triggered.\")\n",
        "            break\n",
        "\n",
        "print(\"\\nBest Val Acc:\", best_val_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_JvY82mAqPeC",
        "outputId": "045b5053-1cff-4820-f57b-84868f488440"
      },
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/10\n",
            "Train Loss: 0.1451 | Train Acc: 0.6760\n",
            "Val   Loss: 0.1216 | Val   Acc: 0.7762\n",
            "âœ… Saved best model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 2/10\n",
            "Train Loss: 0.1095 | Train Acc: 0.8017\n",
            "Val   Loss: 0.1263 | Val   Acc: 0.8021\n",
            "âœ… Saved best model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 3/10\n",
            "Train Loss: 0.0837 | Train Acc: 0.8612\n",
            "Val   Loss: 0.1290 | Val   Acc: 0.8012\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                                      "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 4/10\n",
            "Train Loss: 0.0566 | Train Acc: 0.9040\n",
            "Val   Loss: 0.1693 | Val   Acc: 0.7945\n",
            "ðŸ›‘ Early stopping triggered.\n",
            "\n",
            "Best Val Acc: 0.8021377672209026\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 150,
      "metadata": {
        "id": "3FCOjdrndM4i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "106a63bb-7102-4b0c-bb89-3a305b49b26b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                                      "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Final Val Accuracy: 0.8021377672209026\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Real     0.7856    0.8876    0.8335      2349\n",
            "        Fake     0.8303    0.6943    0.7562      1861\n",
            "\n",
            "    accuracy                         0.8021      4210\n",
            "   macro avg     0.8080    0.7909    0.7949      4210\n",
            "weighted avg     0.8054    0.8021    0.7993      4210\n",
            "\n",
            "Confusion Matrix:\n",
            "[[2085  264]\n",
            " [ 569 1292]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r"
          ]
        }
      ],
      "source": [
        "model.load_state_dict(torch.load(\"best_multimodal_model.pt\", map_location=DEVICE))\n",
        "_, final_acc, y_true, y_pred = eval_one_epoch(model, val_loader)\n",
        "\n",
        "print(\"\\nFinal Val Accuracy:\", final_acc)\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_true, y_pred, target_names=[\"Real\", \"Fake\"], digits=4))\n",
        "\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_true, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "@torch.no_grad()\n",
        "def get_probs(model, loader):\n",
        "    model.eval()\n",
        "    probs, labels = [], []\n",
        "    for batch in loader:\n",
        "        logits = model(\n",
        "            batch[\"image\"].to(DEVICE),\n",
        "            batch[\"input_ids\"].to(DEVICE),\n",
        "            batch[\"attention_mask\"].to(DEVICE)\n",
        "        )\n",
        "        p_fake = F.softmax(logits, dim=1)[:, 1].cpu().numpy()\n",
        "        probs.extend(p_fake)\n",
        "        labels.extend(batch[\"label\"].cpu().numpy())\n",
        "    return np.array(probs), np.array(labels)\n",
        "\n",
        "probs, y_true = get_probs(model, val_loader)\n",
        "\n",
        "best_t, best_f1 = 0.5, 0\n",
        "for t in np.linspace(0.1, 0.9, 81):\n",
        "    y_pred = (probs >= t).astype(int)\n",
        "    f1 = f1_score(y_true, y_pred, pos_label=1)\n",
        "    if f1 > best_f1:\n",
        "        best_f1 = f1\n",
        "        best_t = t\n",
        "\n",
        "print(\"Best threshold:\", best_t)\n",
        "print(\"Best Fake F1:\", best_f1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mcjWgRsbaoR5",
        "outputId": "bdc060de-2f71-467f-f636-e849575f8636"
      },
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best threshold: 0.45000000000000007\n",
            "Best Fake F1: 0.7838199839271364\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "y_pred_thr = (probs >= best_t).astype(int)\n",
        "\n",
        "print(classification_report(y_true, y_pred_thr, target_names=[\"Real\",\"Fake\"], digits=4))\n",
        "print(confusion_matrix(y_true, y_pred_thr))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "csjxdvHqaq7e",
        "outputId": "38ea1196-f25a-4024-92fe-4d0ea9708137"
      },
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Real     0.8298    0.8259    0.8278      2349\n",
            "        Fake     0.7815    0.7861    0.7838      1861\n",
            "\n",
            "    accuracy                         0.8083      4210\n",
            "   macro avg     0.8056    0.8060    0.8058      4210\n",
            "weighted avg     0.8084    0.8083    0.8084      4210\n",
            "\n",
            "[[1940  409]\n",
            " [ 398 1463]]\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyNAhXHem4igDLH5VTH3UxZv",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}